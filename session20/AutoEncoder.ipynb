{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The neuron non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0,z)\n",
    "    \n",
    "def ReLUPrime(z):\n",
    "    return 1. if z > 0. else 0.\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def sigmoidPrime(z):\n",
    "    # Derivative of Sigmoid Function\n",
    "    # dy/dz = e^-z / (1+e^-z)^2 = y * (1-y)\n",
    "    return np.exp(-z) / ((1+np.exp(-z))**2)\n",
    "\n",
    "# apply the function on every element of a vector\n",
    "ReLU = np.vectorize(ReLU)\n",
    "ReLUPrime = np.vectorize(ReLUPrime)\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "sigmoidPrime = np.vectorize(sigmoidPrime)\n",
    "\n",
    "f = ReLU\n",
    "fPrime = ReLUPrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-6, 6, 0.01)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(12,4))\n",
    "ax1.plot(x, ReLU(x), label=\"ReLU\", color=\"blue\")\n",
    "ax1.plot(x, ReLUPrime(x), label=\"ReLU'\", color=\"cyan\")\n",
    "ax1.legend(loc=2)\n",
    "\n",
    "ax2.plot(x, sigmoid(x), label=\"sigmoid\", color=\"red\")\n",
    "ax2.plot(x, sigmoidPrime(x), label=\"sigmoid'\", color=\"orange\")\n",
    "ax2.legend(loc=2)\n",
    "\n",
    "plt.ylim(-.1,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    x = np.random.uniform(low=0.0, high=50.0)\n",
    "    y = np.random.uniform(low=0.0, high=30.0)\n",
    "    z_1 = 0.25 * x + 0.75 * y + np.random.normal(loc=0.0, scale=0.01)\n",
    "    z_2 = .33333 * y + .66666 * x + np.random.normal(loc=0.0, scale=0.01)\n",
    "    return np.array([x, z_1, y, z_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "data = [ sample() for i in xrange(0,100) ]\n",
    "xs = [ s[0] for s in data ]\n",
    "ys = [ s[2] for s in data ]\n",
    "zs1 = [0.25 * x + 0.75 * y for (x,y) in zip(xs,ys)]\n",
    "zs1_ = [ s[1] for s in data ]\n",
    "zs2 = [.33333 * y + .66666 * x for (x,y) in zip(xs,ys)]\n",
    "zs2_ = [ s[3] for s in data ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(xs, ys, zs=zs1, s=5, marker='+', color=\"red\")\n",
    "ax.scatter(xs, ys, zs=zs1_, s=5, marker='+', color=\"orange\")\n",
    "ax.scatter(xs, ys, zs=zs2, s=5, marker='*', color=\"blue\")\n",
    "ax.scatter(xs, ys, zs=zs2_, s=5, marker='*', color=\"cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Error\n",
    "\n",
    "$error(target, output) = \\frac{1}{2} \\sum\\limits_{i}{(target_i-output_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(target, output):\n",
    "    return .5 * np.sum(map(lambda (t, y): (t-y)**2, zip(target, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer containing neurons and weights from prev layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, N_in, N, norm=True, weight_scale=0.01, with_bias=False):\n",
    "        self.name = name\n",
    "        self.N = N\n",
    "        self.N_in = N_in\n",
    "        \n",
    "        # summed input\n",
    "        self.Z = np.zeros(N)\n",
    "        self.E_Z = np.zeros(N)\n",
    "        \n",
    "        # and applied non-linearity\n",
    "        self.Y = np.zeros(N)\n",
    "        self.E_Y = np.zeros(N)\n",
    "        \n",
    "        # add bias weights, if requested\n",
    "        bias = 1 if with_bias else 0\n",
    "        # initialize incomming weights\n",
    "        if norm:\n",
    "            self.W = np.random.normal(loc=0.0, scale=weight_scale, size=(N_in + bias, N))\n",
    "        else:\n",
    "            self.W = np.random.uniform(low=0.0, high=weight_scale, size=(N_in + bias, N))\n",
    "        self.E_W = np.zeros(self.W.shape)\n",
    "        # store last velocity\n",
    "        self.V = np.zeros(self.W.shape)\n",
    "        \n",
    "    def reset_error(self):\n",
    "        self.E_Y *= 0\n",
    "        self.E_Z *= 0\n",
    "        self.E_W *= 0\n",
    "    \n",
    "    def write(self):\n",
    "        print\"Name: \", self.name\n",
    "        print\"Y   :\\n\", self.Y\n",
    "        print\"E_Y :\\n\", self.E_Y\n",
    "        print\"Z   :\\n\", self.Z\n",
    "        print\"E_Z :\\n\", self.E_Z\n",
    "        print\"W   :\\n\", self.W\n",
    "        print\"E_W :\\n\", self.E_W\n",
    "\n",
    "    def plot(self):\n",
    "        f, axes = plt.subplots(2, 1)\n",
    "        (axW, axY) = axes\n",
    "        axW.imshow(self.W,\n",
    "                   cmap=\"gray\",\n",
    "                   interpolation='none',\n",
    "                   vmax=np.max(self.W), vmin=np.min(self.W))\n",
    "        axY.imshow([self.Y],\n",
    "                   cmap=\"gray\",\n",
    "                   interpolation='none',\n",
    "                   vmax=np.max(self.Y), vmin=np.min(self.Y))\n",
    "        for a in axes:\n",
    "            a.get_xaxis().set_visible(False)\n",
    "            a.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(layers, training, with_bias=False):\n",
    "    \"\"\"\n",
    "    layers:    np.array with multiple layers\n",
    "    training:  np.array with one training case\n",
    "    with_bias: boolean  if true add 'bias neuron'\n",
    "    \"\"\"\n",
    "    I = training\n",
    "    if with_bias:\n",
    "        I = np.append(I, [1.0])\n",
    "    for l in layers:\n",
    "        l.Z = I.dot(l.W)\n",
    "        l.Y = f(l.Z)\n",
    "        I = l.Y\n",
    "        if with_bias:\n",
    "            I = np.append(I, [1.0])\n",
    "    \n",
    "    return I[0:len(layers[-1].Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Path and Error Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(layers, training, with_bias=False):\n",
    "    \"\"\"\n",
    "    layers:    np.array with multiple layers\n",
    "    training:  np.array with one training case\n",
    "    with_bias: boolean  if true add 'bias neuron'\n",
    "    \"\"\"\n",
    "    output = layers[1]\n",
    "    hidden = layers[0]\n",
    "    # Output layer\n",
    "    # dE/dY_O = (-(target - output)) = output - target\n",
    "    output.E_Y = output.Y - training\n",
    "    # dE/dZ_O = dE/dY_O * dY_O/dZ_O\n",
    "    output.E_Z = output.E_Y * fPrime(output.Z)\n",
    "    # dE/dW2 = Y_H * dE/dZ_O\n",
    "    # mini-batch\n",
    "    Y = hidden.Y\n",
    "    if with_bias:\n",
    "        Y = np.append(Y, [1.0])\n",
    "    output.E_W += np.transpose([Y]) * [output.E_Z]\n",
    "    \n",
    "    # Hidden layer\n",
    "    # dE/dY_H = dE/dZ_O * trans(W2)\n",
    "    hidden.E_Y = output.E_Z.dot(np.transpose(output.W))\n",
    "    # dE/dZ_H = dE/dY_H * dY_H/dZ_H\n",
    "    hidden.E_Z = hidden.E_Y[0:len(hidden.Z)] * fPrime(hidden.Z)\n",
    "    # dE/dW1 = Y_I * dE/dZ_H\n",
    "    # mini-batch\n",
    "    Y = training\n",
    "    if with_bias:\n",
    "        Y = np.append(Y, [1.0])\n",
    "    hidden.E_W += np.transpose([Y]) * [hidden.E_Z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum SGD Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(layers, eps, batch_size, momentum):\n",
    "    \"\"\"\n",
    "    layers:     np.array with multiple layers\n",
    "    eps:        double   learning rate\n",
    "    batch_size: integer  number of training cases in last batch\n",
    "    momentum:   double   momentum rate (0..1)\n",
    "    \"\"\"\n",
    "    for l in layers:\n",
    "        # adjust old velocity    and go into the new gradient\n",
    "        V_curr = momentum * l.V - eps * l.E_W / batch_size\n",
    "        # apply new velocity\n",
    "        l.W = l.W + V_curr\n",
    "        # store for next learn\n",
    "        l.V = V_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(layers, targets, eps, momentum, with_bias=False):\n",
    "    \"\"\"\n",
    "    Do one batch learning step.\n",
    "\n",
    "    layers:     np.array with multiple layers\n",
    "    targets:    np.array with multiple training causes\n",
    "    eps:        double   learning rate\n",
    "    momentum:   double   momentum rate (0..1)\n",
    "    with_bias:  boolean  if true add 'bias neuron'\n",
    "    \"\"\"\n",
    "    # accumulate the average error\n",
    "    err = 0.0\n",
    "    for target in targets:\n",
    "        # for each target do forward step\n",
    "        output = forward( layers, target, with_bias )\n",
    "        # and estimate gradient\n",
    "        backward( layers, target, with_bias )\n",
    "        # sum errors\n",
    "        err += error(target, output)\n",
    "    \n",
    "    # do one learn step with the summed gradients\n",
    "    learn(layers, eps, len(targets), momentum)\n",
    "    \n",
    "    for l in layers:\n",
    "        l.reset_error()\n",
    "\n",
    "    return err / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov's Accelerated Gradient\n",
    "\n",
    "Similar to momentum SGD, but first do the velocity/momentum step,\n",
    "then measure the gradient at this point and correct the new error.\n",
    "\n",
    "<img src=\"NAG.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG_batch(layers, targets, eps, momentum, with_bias=False):\n",
    "    # first jump into prev. direction\n",
    "    V_curr = []\n",
    "    for l in layers:\n",
    "        V_tmp = momentum * l.V\n",
    "        l.W = l.W + V_tmp\n",
    "        V_curr.append(V_tmp)\n",
    "\n",
    "    # measure gradients\n",
    "    err = 0.0\n",
    "    for target in targets:\n",
    "        output = forward( layers, target, with_bias )\n",
    "        backward( layers, target, with_bias )\n",
    "        err += error(target, output)\n",
    "    \n",
    "    # apply gradient\n",
    "    for l, v in zip(layers, V_curr):\n",
    "        G_tmp = eps * l.E_W / len(targets)\n",
    "        l.W = l.W - G_tmp\n",
    "        l.V = v - G_tmp\n",
    "    \n",
    "    for l in layers:\n",
    "        l.reset_error()\n",
    "\n",
    "    return err / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AutoEncoder\n",
    "\n",
    "<img src=\"AutoEncoder.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "norm = False\n",
    "use_bias = True\n",
    "h = Layer(\"hidden\", 4, 2, norm, scale, use_bias)\n",
    "o = Layer(\"output\", 2, 4, norm, scale, use_bias)\n",
    "layers = [h,o]\n",
    "\n",
    "# Learning\n",
    "learn_errors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 0.00001\n",
    "momentum = 0.99\n",
    "\n",
    "try:\n",
    "    for i in xrange(0, 10000):\n",
    "        if i % 19 == 0:\n",
    "            plt.clf()\n",
    "            plt.plot(xrange(0, len(learn_errors)),\n",
    "                     learn_errors, color='blue')\n",
    "            plt.yscale('log')\n",
    "            if i > 100:\n",
    "                plt.axhline(y=np.mean(learn_errors[-100:-1]), color=\"red\")\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "        targets = [ sample() for i in xrange(0,100) ]\n",
    "        err = NAG_batch(layers, targets, eps=eps, momentum=momentum, with_bias=use_bias)\n",
    "        #err = batch(layers, targets, eps=eps, momentum=momentum, with_bias=use_bias)\n",
    "\n",
    "        learn_errors += [err]\n",
    "        if np.mean(learn_errors[-100:-1]) < 1.e-10:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print \"Interrupted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = sample()\n",
    "o = forward(layers, t, use_bias)\n",
    "for l in reversed(layers):\n",
    "    l.reset_error()\n",
    "    l.write()\n",
    "    print \"\"\n",
    "print \"Input :\", t\n",
    "print \"Output:\", o\n",
    "print \"Diff  :\", o - t\n",
    "print \"Error :\", error(t, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow([t],\n",
    "        cmap=\"gray\",\n",
    "        interpolation='none',\n",
    "        vmax=np.max(t), vmin=np.min(t))\n",
    "plt.axes().get_xaxis().set_visible(False)\n",
    "plt.axes().get_yaxis().set_visible(False)\n",
    "for l in layers:\n",
    "    l.reset_error()\n",
    "    l.plot()\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_errors = []\n",
    "for i in xrange(0, 1000):\n",
    "    target = sample()\n",
    "    output = forward( layers, target, use_bias )\n",
    "    err = error(target, output) \n",
    "    validate_errors += [err]\n",
    "\n",
    "plt.plot(xrange(0, len(validate_errors)), validate_errors)\n",
    "plt.axhline(y=np.mean(validate_errors), color=\"red\")\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
